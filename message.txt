Deep Reinforcement Learning (DRL) enables agents to make decisions based on a well-designed reward function that suits a particular environment without any prior knowledge related to a given environment. The adaptation of hyperparameters has a great impact on the overall learning process and the learning processing times. Hyperparameters should be accurately estimated while training DRL algorithms, which is one of the key challenges that we attempt to address.
The lunar lander task is based on the open source physics engine Box2D. DQN(Deep Q-Network) is a popular algorithm that trains agents in reinforcement learning. We attempt to use the DQN method to make the agent land successfully in the lunar lander task. There are some hyperparameters in the DQN that make it more sensitive. This makes it difficult to converge fast and stable during training.
Here, we ask how we can change the hyperparameter to improve the DQN performance while keeping the stability of that, and which hyperparameters are  the most important for speed of convergence? We hypothesized that the learning rate, discounted factor, number of layers, reward function, batch size, exploration-exploitation rate in DQN are important in speed of convergence and algorithm performance.
To investigate these hypotheses, we implemented a simple DQN algorithm on the Lunar Lander environment using a simple 4 layer convolutional neural network (CNN) and trained it on different subsets of different hyperparameters. We investigate the effect of each hyperparameter on the performance of the algorithm with compare average rewards and variance of rewards.
We found that the defined hyperparameters affect the performance of the algorithm. For example the bigger learning rate decreases the speed of DQN. Also, the speed and stability of the DQN are affected by discounted-factor and exploration-exploitation rate. 
Since our model is an ideal and simple model, we don’t consider the difference of reward function to the performance of the agent. Besides, we don’t consider other factors that affect the speed and stability of landing, like obstacles, wind and gas.
Further, we can attempt to use other extensions, like Double DQN, Dueling DQN, Rainbow to train agent land successfully, making it land fast and stable. And we take  more factors into consideration in the model.

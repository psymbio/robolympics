{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a90bf556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['- HumanoidDeepMimicBackflipBulletEnv-v1', '- HumanoidDeepMimicWalkBulletEnv-v1', '- CartPoleBulletEnv-v1', '- CartPoleContinuousBulletEnv-v0', '- MinitaurBulletEnv-v0', '- MinitaurBulletDuckEnv-v0', '- RacecarBulletEnv-v0', '- RacecarZedBulletEnv-v0', '- KukaBulletEnv-v0', '- KukaCamBulletEnv-v0', '- InvertedPendulumBulletEnv-v0', '- InvertedDoublePendulumBulletEnv-v0', '- InvertedPendulumSwingupBulletEnv-v0', '- ReacherBulletEnv-v0', '- PusherBulletEnv-v0', '- ThrowerBulletEnv-v0', '- Walker2DBulletEnv-v0', '- HalfCheetahBulletEnv-v0', '- AntBulletEnv-v0', '- HopperBulletEnv-v0', '- HumanoidBulletEnv-v0', '- HumanoidFlagrunBulletEnv-v0', '- HumanoidFlagrunHarderBulletEnv-v0', '- MinitaurExtendedEnv-v0', '- MinitaurReactiveEnv-v0', '- MinitaurBallGymEnv-v0', '- MinitaurTrottingEnv-v0', '- MinitaurStandGymEnv-v0', '- MinitaurAlternatingLegsEnv-v0', '- MinitaurFourLegStandEnv-v0', '- KukaDiverseObjectGrasping-v0']\n",
      "1000.0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vanillaskies/anaconda3/envs/mne/lib/python3.9/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib\n",
    "import pybullet_envs2\n",
    "\n",
    "from acme.utils import loggers\n",
    "from acme.tf import networks\n",
    "from acme.tf import utils as tf2_utils\n",
    "from acme.agents.tf.d4pg import D4PG\n",
    "from acme.agents.tf.ddpg import DDPG\n",
    "from acme.agents.tf.dmpo import DistributionalMPO\n",
    "from acme import wrappers, specs, environment_loop\n",
    "\n",
    "import numpy as np\n",
    "import sonnet as snt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# from google.colab import drive\n",
    "from IPython.display import HTML\n",
    "\n",
    "from pybullet_envs2.gym_locomotion_envs import HopperBulletEnv\n",
    "from pybullet_envs2.gym_locomotion_envs import Walker2DBulletEnv\n",
    "from pybullet_envs2.gym_locomotion_envs import HalfCheetahBulletEnv\n",
    "from pybullet_envs2.gym_locomotion_envs import AntBulletEnv\n",
    "from pybullet_envs2.gym_locomotion_envs import HumanoidBulletEnv, HumanoidBulletEnv2\n",
    "\n",
    "# perfect our own instance of the enviroment is created\n",
    "# time to manipulate the environments\n",
    "print(pybullet_envs2.getList())\n",
    "\n",
    "# @title Figure settings\n",
    "import ipywidgets as widgets       # interactive display\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle\")\n",
    "\n",
    "\n",
    "def save_ckpt_to_drive(agent):\n",
    "  \"\"\"Saves agent checkpoint directory to Google Drive.\n",
    "\n",
    "  WARNING: Will replace the entire content of the\n",
    "  drive directory `/root/drive/MyDrive/acme_ckpt`.\n",
    "\n",
    "  Args:\n",
    "    agent: core.Actor\n",
    "  \"\"\"\n",
    "  src = agent._learner._checkpointer._checkpoint_manager.directory\n",
    "  dst = '/root/drive/MyDrive/acme_ckpt'\n",
    "  if os.path.exists(dst):\n",
    "    shutil.rmtree(dst)\n",
    "  shutil.copytree(src, dst)\n",
    "  print(f'Saved {src} to {dst}')\n",
    "\n",
    "\n",
    "def restore_ckpt_from_drive(agent):\n",
    "  \"\"\"Restores agent checkpoint directory from Google Drive.\n",
    "\n",
    "  The name of the local checkpoint directory will be different\n",
    "  than it was when the checkpoint was originally saved.\n",
    "  This is because `acme` checkpoiner creates a new directory\n",
    "  upon restart.\n",
    "\n",
    "  WARNING: Will replace the entire content of the local\n",
    "  checkpoint directory (if it exists already).\n",
    "\n",
    "  Args:\n",
    "    agent: core.Actor\n",
    "  \"\"\"\n",
    "  src = '/root/drive/MyDrive/acme_ckpt'\n",
    "  dst = agent._learner._checkpointer._checkpoint_manager.directory\n",
    "  if os.path.exists(dst):\n",
    "        shutil.rmtree(dst)\n",
    "  shutil.copytree(src, dst)\n",
    "  print(f'Restored {dst} from {src}')\n",
    "    \n",
    "def display_video(frames, framerate=30):\n",
    "  \"\"\"Generates video from `frames`.\n",
    "\n",
    "  Args:\n",
    "    frames (ndarray): Array of shape (n_frames, height, width, 3).\n",
    "    framerate (int): Frame rate in units of Hz.\n",
    "\n",
    "  Returns:\n",
    "    Display object.\n",
    "  \"\"\"\n",
    "  height, width, _ = frames[0].shape\n",
    "  dpi = 70\n",
    "  orig_backend = matplotlib.get_backend()\n",
    "  matplotlib.use('Agg')  # Switch to headless 'Agg' to inhibit figure rendering.\n",
    "  fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
    "  matplotlib.use(orig_backend)  # Switch back to the original backend.\n",
    "  ax.set_axis_off()\n",
    "  ax.set_aspect('equal')\n",
    "  ax.set_position([0, 0, 1, 1])\n",
    "  im = ax.imshow(frames[0])\n",
    "  def update(frame):\n",
    "    im.set_data(frame)\n",
    "    return [im]\n",
    "  interval = 1000/framerate\n",
    "  anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
    "                                  interval=interval, blit=True, repeat=False)\n",
    "  return HTML(anim.to_html5_video())\n",
    "\n",
    "def make_networks_d4pg(action_spec,\n",
    "                       policy_layer_sizes=(256, 256, 256),\n",
    "                       critic_layer_sizes=(512, 512, 256),\n",
    "                       vmin=-150.,\n",
    "                       vmax=150.,\n",
    "                       num_atoms=51,\n",
    "                      ):\n",
    "  \"\"\"Networks for D4PG agent.\"\"\"\n",
    "  action_size = np.prod(action_spec.shape, dtype=int)\n",
    "\n",
    "  policy_network = snt.Sequential([\n",
    "      tf2_utils.batch_concat,\n",
    "      networks.LayerNormMLP(layer_sizes=policy_layer_sizes + (action_size,)),\n",
    "      networks.TanhToSpec(spec=action_spec)\n",
    "      ])\n",
    "  critic_network = snt.Sequential([\n",
    "      networks.CriticMultiplexer(\n",
    "          action_network=networks.ClipToSpec(action_spec),\n",
    "          critic_network=networks.LayerNormMLP(\n",
    "              layer_sizes=critic_layer_sizes,\n",
    "              activate_final=True),\n",
    "      ),\n",
    "      networks.DiscreteValuedHead(vmin=vmin,\n",
    "                                  vmax=vmax,\n",
    "                                  num_atoms=num_atoms)\n",
    "      ])\n",
    "\n",
    "  return policy_network, critic_network\n",
    "\n",
    "\n",
    "def make_networks_ddpg(action_spec,\n",
    "                       policy_layer_sizes=(256, 256, 256),\n",
    "                       critic_layer_sizes=(512, 512, 256),\n",
    "                      ):\n",
    "  \"\"\"Networks for DDPG agent.\"\"\"\n",
    "  action_size = np.prod(action_spec.shape, dtype=int)\n",
    "\n",
    "  policy_network = snt.Sequential([\n",
    "      tf2_utils.batch_concat,\n",
    "      networks.LayerNormMLP(layer_sizes=policy_layer_sizes + (action_size,)),\n",
    "      networks.TanhToSpec(spec=action_spec)\n",
    "      ])\n",
    "  critic_network = networks.CriticMultiplexer(\n",
    "          action_network=networks.ClipToSpec(action_spec),\n",
    "          critic_network=networks.LayerNormMLP(\n",
    "              layer_sizes=critic_layer_sizes + (1,),\n",
    "              activate_final=False),\n",
    "              )\n",
    "\n",
    "  return policy_network, critic_network\n",
    "\n",
    "\n",
    "def make_networks_dmpo(action_spec,\n",
    "                       policy_layer_sizes=(256, 256, 256),\n",
    "                       critic_layer_sizes=(512, 512, 256),\n",
    "                       vmin=-150.,\n",
    "                       vmax=150.,\n",
    "                       num_atoms=51,\n",
    "                      ):\n",
    "  \"\"\"Networks for DMPO agent.\"\"\"\n",
    "  action_size = np.prod(action_spec.shape, dtype=int)\n",
    "\n",
    "  policy_network = snt.Sequential([\n",
    "      tf2_utils.batch_concat,\n",
    "      networks.LayerNormMLP(layer_sizes=policy_layer_sizes,\n",
    "                            activate_final=True),\n",
    "      networks.MultivariateNormalDiagHead(\n",
    "          action_size,\n",
    "          min_scale=1e-6,\n",
    "          tanh_mean=False,\n",
    "          init_scale=0.7,\n",
    "          fixed_scale=False,\n",
    "          use_tfd_independent=True)\n",
    "  ])\n",
    "\n",
    "  # The multiplexer concatenates the (maybe transformed) observations/actions.\n",
    "  critic_network = networks.CriticMultiplexer(\n",
    "      action_network=networks.ClipToSpec(action_spec),\n",
    "      critic_network=networks.LayerNormMLP(layer_sizes=critic_layer_sizes,\n",
    "                                           activate_final=True),\n",
    "                                           )\n",
    "  critic_network = snt.Sequential([\n",
    "                                   critic_network,\n",
    "                                   networks.DiscreteValuedHead(vmin=vmin,\n",
    "                                                               vmax=vmax,\n",
    "                                                               num_atoms=num_atoms)\n",
    "                                   ])\n",
    "\n",
    "  return policy_network, critic_network\n",
    "\n",
    "\n",
    "class Humanoid2(HumanoidBulletEnv2):\n",
    "\n",
    "  def __init__(self, render=False, episode_steps=1000):\n",
    "    \"\"\"Modifies `__init__` in `HopperBulletEnv` parent class.\"\"\"\n",
    "    self.episode_steps = episode_steps\n",
    "    super().__init__(render=render)\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"Modifies `reset` in `WalkerBaseBulletEnv` base class.\"\"\"\n",
    "    self.step_counter = 0\n",
    "    return super().reset()\n",
    "\n",
    "  def _isDone(self):\n",
    "    \"\"\"Modifies `_isDone` in `WalkerBaseBulletEnv` base class.\"\"\"\n",
    "    return (self.step_counter == self.episode_steps\n",
    "            or super()._isDone())\n",
    "\n",
    "  def step(self, a):\n",
    "    \"\"\"Fully overrides `step` in `WalkerBaseBulletEnv` base class.\"\"\"\n",
    "\n",
    "    self.step_counter += 1\n",
    "\n",
    "    # if multiplayer, action first applied to all robots,\n",
    "    # then global step() called, then _step() for all robots\n",
    "    # with the same actions\n",
    "    if not self.scene.multiplayer:\n",
    "      self.robot.apply_action(a)\n",
    "      self.scene.global_step()\n",
    "\n",
    "    state = self.robot.calc_state()  # also calculates self.joints_at_limit\n",
    "\n",
    "    # state[0] is body height above ground, body_rpy[1] is pitch\n",
    "    self._alive = float(self.robot.alive_bonus(state[0] + self.robot.initial_z,\n",
    "                                               self.robot.body_rpy[1]))\n",
    "    done = self._isDone()\n",
    "    if not np.isfinite(state).all():\n",
    "      print(\"~INF~\", state)\n",
    "      done = True\n",
    "\n",
    "    potential_old = self.potential\n",
    "    self.potential = self.robot.calc_potential()\n",
    "    progress = float(self.potential - potential_old)\n",
    "\n",
    "    feet_collision_cost = 0.0\n",
    "    for i, f in enumerate(self.robot.feet):\n",
    "      contact_ids = set((x[2], x[4]) for x in f.contact_list())\n",
    "      if (self.ground_ids & contact_ids):\n",
    "        self.robot.feet_contact[i] = 1.0\n",
    "      else:\n",
    "        self.robot.feet_contact[i] = 0.0\n",
    "\n",
    "    # let's assume we have DC motor with controller, and reverse current braking\n",
    "    electricity_cost = self.electricity_cost * float(\n",
    "        np.abs(a * self.robot.joint_speeds).mean())\n",
    "    electricity_cost += self.stall_torque_cost * float(np.square(a).mean())\n",
    "\n",
    "    joints_at_limit_cost = float(self.joints_at_limit_cost * self.robot.joints_at_limit)\n",
    "\n",
    "    self.rewards = [\n",
    "                    self._alive, progress, electricity_cost,\n",
    "                    joints_at_limit_cost, feet_collision_cost\n",
    "                    ]\n",
    "    self.HUD(state, a, done)\n",
    "    self.reward += sum(self.rewards)\n",
    "\n",
    "    return state, sum(self.rewards), bool(done), {}\n",
    "env = Humanoid2()\n",
    "env = wrappers.GymWrapper(env)\n",
    "env = wrappers.SinglePrecisionWrapper(env)\n",
    "action_spec = env.action_spec()  # Specifies action shape and dimensions.\n",
    "env_spec = specs.make_environment_spec(env)\n",
    "print(env.robot.walk_target_x, env.robot.walk_target_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c3467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = env.reset()\n",
    "\n",
    "# frame = env.environment.render(mode='rgb_array')\n",
    "# plt.imshow(frame)\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb38614",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 200\n",
    "\n",
    "frames = []\n",
    "timestep = env.reset()\n",
    "for _ in range(n_steps):\n",
    "  # Random control of actuators.\n",
    "  action = np.random.uniform(action_spec.minimum,\n",
    "                             action_spec.maximum,\n",
    "                             size=action_spec.shape)\n",
    "  timestep = env.step(action)\n",
    "  frames.append(env.environment.render(mode='rgb_array'))\n",
    "\n",
    "display_video(frames, framerate=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

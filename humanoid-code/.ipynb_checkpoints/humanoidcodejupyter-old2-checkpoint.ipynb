{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62870f55",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'acme.utils.loggers.auto_close'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-aeb3e0aa960a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpybullet_envs2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0macme2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloggers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0macme2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/nma2/robolympics-git/humanoid-code/acme2/acme/utils/loggers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0macme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloggers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDispatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0macme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloggers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAsyncLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0macme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloggers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_close\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoCloseLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0macme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloggers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0macme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloggers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoggingData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'acme.utils.loggers.auto_close'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib\n",
    "import pybullet_envs2\n",
    "\n",
    "# from acme2.acme.utils import loggers\n",
    "# from acme2.tf import networks\n",
    "\n",
    "# from acme2.tf import utils as tf2_utils\n",
    "# from acme2.agents.tf.d4pg import D4PG\n",
    "# from acme2.agents.tf.ddpg import DDPG\n",
    "# from acme2.agents.tf.dmpo import DistributionalMPO\n",
    "# from acme2.acme import wrappers, specs, environment_loop\n",
    "\n",
    "from acme.utils import loggers\n",
    "from acme.tf import networks\n",
    "from acme.tf import utils as tf2_utils\n",
    "from acme.agents.tf.d4pg import D4PG\n",
    "from acme.agents.tf.ddpg import DDPG\n",
    "from acme.agents.tf.dmpo import DistributionalMPO\n",
    "from acme import wrappers, specs, environment_loop\n",
    "\n",
    "import numpy as np\n",
    "import sonnet as snt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# from google.colab import drive\n",
    "from IPython.display import HTML\n",
    "\n",
    "from pybullet_envs2.gym_locomotion_envs import HopperBulletEnv\n",
    "from pybullet_envs2.gym_locomotion_envs import Walker2DBulletEnv\n",
    "from pybullet_envs2.gym_locomotion_envs import HalfCheetahBulletEnv\n",
    "from pybullet_envs2.gym_locomotion_envs import AntBulletEnv\n",
    "from pybullet_envs2.gym_locomotion_envs import HumanoidBulletEnv, HumanoidBulletEnv2\n",
    "from pybullet_envs2.gym_locomotion_envs import HumanoidBulletEnv3\n",
    "\n",
    "# perfect our own instance of the enviroment is created\n",
    "# time to manipulate the environments\n",
    "print(pybullet_envs2.getList())\n",
    "\n",
    "# @title Figure settings\n",
    "import ipywidgets as widgets       # interactive display\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle\")\n",
    "\n",
    "\n",
    "def save_ckpt_to_drive(agent):\n",
    "  \"\"\"Saves agent checkpoint directory to Google Drive.\n",
    "\n",
    "  WARNING: Will replace the entire content of the\n",
    "  drive directory `/root/drive/MyDrive/acme_ckpt`.\n",
    "\n",
    "  Args:\n",
    "    agent: core.Actor\n",
    "  \"\"\"\n",
    "  src = agent._learner._checkpointer._checkpoint_manager.directory\n",
    "  dst = '/root/drive/MyDrive/acme_ckpt'\n",
    "  if os.path.exists(dst):\n",
    "    shutil.rmtree(dst)\n",
    "  shutil.copytree(src, dst)\n",
    "  print(f'Saved {src} to {dst}')\n",
    "\n",
    "\n",
    "def restore_ckpt_from_drive(agent):\n",
    "  \"\"\"Restores agent checkpoint directory from Google Drive.\n",
    "\n",
    "  The name of the local checkpoint directory will be different\n",
    "  than it was when the checkpoint was originally saved.\n",
    "  This is because `acme` checkpoiner creates a new directory\n",
    "  upon restart.\n",
    "\n",
    "  WARNING: Will replace the entire content of the local\n",
    "  checkpoint directory (if it exists already).\n",
    "\n",
    "  Args:\n",
    "    agent: core.Actor\n",
    "  \"\"\"\n",
    "  src = '/root/drive/MyDrive/acme_ckpt'\n",
    "  dst = agent._learner._checkpointer._checkpoint_manager.directory\n",
    "  if os.path.exists(dst):\n",
    "        shutil.rmtree(dst)\n",
    "  shutil.copytree(src, dst)\n",
    "  print(f'Restored {dst} from {src}')\n",
    "    \n",
    "def display_video(frames, framerate=30):\n",
    "  \"\"\"Generates video from `frames`.\n",
    "\n",
    "  Args:\n",
    "    frames (ndarray): Array of shape (n_frames, height, width, 3).\n",
    "    framerate (int): Frame rate in units of Hz.\n",
    "\n",
    "  Returns:\n",
    "    Display object.\n",
    "  \"\"\"\n",
    "  height, width, _ = frames[0].shape\n",
    "  dpi = 70\n",
    "  orig_backend = matplotlib.get_backend()\n",
    "  matplotlib.use('Agg')  # Switch to headless 'Agg' to inhibit figure rendering.\n",
    "  fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
    "  matplotlib.use(orig_backend)  # Switch back to the original backend.\n",
    "  ax.set_axis_off()\n",
    "  ax.set_aspect('equal')\n",
    "  ax.set_position([0, 0, 1, 1])\n",
    "  im = ax.imshow(frames[0])\n",
    "  def update(frame):\n",
    "    im.set_data(frame)\n",
    "    return [im]\n",
    "  interval = 1000/framerate\n",
    "  anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
    "                                  interval=interval, blit=True, repeat=False)\n",
    "  return HTML(anim.to_html5_video())\n",
    "\n",
    "def make_networks_d4pg(action_spec,\n",
    "                       policy_layer_sizes=(256, 256, 256),\n",
    "                       critic_layer_sizes=(512, 512, 256),\n",
    "                       vmin=-150.,\n",
    "                       vmax=150.,\n",
    "                       num_atoms=51,\n",
    "                      ):\n",
    "  \"\"\"Networks for D4PG agent.\"\"\"\n",
    "  action_size = np.prod(action_spec.shape, dtype=int)\n",
    "\n",
    "  policy_network = snt.Sequential([\n",
    "      tf2_utils.batch_concat,\n",
    "      networks.LayerNormMLP(layer_sizes=policy_layer_sizes + (action_size,)),\n",
    "      networks.TanhToSpec(spec=action_spec)\n",
    "      ])\n",
    "  critic_network = snt.Sequential([\n",
    "      networks.CriticMultiplexer(\n",
    "          action_network=networks.ClipToSpec(action_spec),\n",
    "          critic_network=networks.LayerNormMLP(\n",
    "              layer_sizes=critic_layer_sizes,\n",
    "              activate_final=True),\n",
    "      ),\n",
    "      networks.DiscreteValuedHead(vmin=vmin,\n",
    "                                  vmax=vmax,\n",
    "                                  num_atoms=num_atoms)\n",
    "      ])\n",
    "\n",
    "  return policy_network, critic_network\n",
    "\n",
    "\n",
    "def make_networks_ddpg(action_spec,\n",
    "                       policy_layer_sizes=(256, 256, 256),\n",
    "                       critic_layer_sizes=(512, 512, 256),\n",
    "                      ):\n",
    "  \"\"\"Networks for DDPG agent.\"\"\"\n",
    "  action_size = np.prod(action_spec.shape, dtype=int)\n",
    "\n",
    "  policy_network = snt.Sequential([\n",
    "      tf2_utils.batch_concat,\n",
    "      networks.LayerNormMLP(layer_sizes=policy_layer_sizes + (action_size,)),\n",
    "      networks.TanhToSpec(spec=action_spec)\n",
    "      ])\n",
    "  critic_network = networks.CriticMultiplexer(\n",
    "          action_network=networks.ClipToSpec(action_spec),\n",
    "          critic_network=networks.LayerNormMLP(\n",
    "              layer_sizes=critic_layer_sizes + (1,),\n",
    "              activate_final=False),\n",
    "              )\n",
    "\n",
    "  return policy_network, critic_network\n",
    "\n",
    "\n",
    "def make_networks_dmpo(action_spec,\n",
    "                       policy_layer_sizes=(256, 256, 256),\n",
    "                       critic_layer_sizes=(512, 512, 256),\n",
    "                       vmin=-150.,\n",
    "                       vmax=150.,\n",
    "                       num_atoms=51,\n",
    "                      ):\n",
    "  \"\"\"Networks for DMPO agent.\"\"\"\n",
    "  action_size = np.prod(action_spec.shape, dtype=int)\n",
    "\n",
    "  policy_network = snt.Sequential([\n",
    "      tf2_utils.batch_concat,\n",
    "      networks.LayerNormMLP(layer_sizes=policy_layer_sizes,\n",
    "                            activate_final=True),\n",
    "      networks.MultivariateNormalDiagHead(\n",
    "          action_size,\n",
    "          min_scale=1e-6,\n",
    "          tanh_mean=False,\n",
    "          init_scale=0.7,\n",
    "          fixed_scale=False,\n",
    "          use_tfd_independent=True)\n",
    "  ])\n",
    "\n",
    "  # The multiplexer concatenates the (maybe transformed) observations/actions.\n",
    "  critic_network = networks.CriticMultiplexer(\n",
    "      action_network=networks.ClipToSpec(action_spec),\n",
    "      critic_network=networks.LayerNormMLP(layer_sizes=critic_layer_sizes,\n",
    "                                           activate_final=True),\n",
    "                                           )\n",
    "  critic_network = snt.Sequential([\n",
    "                                   critic_network,\n",
    "                                   networks.DiscreteValuedHead(vmin=vmin,\n",
    "                                                               vmax=vmax,\n",
    "                                                               num_atoms=num_atoms)\n",
    "                                   ])\n",
    "\n",
    "  return policy_network, critic_network\n",
    "\n",
    "\n",
    "class HumanoidURDF(HumanoidBulletEnv3):\n",
    "\n",
    "  def __init__(self, render=False, episode_steps=1000):\n",
    "    \"\"\"Modifies `__init__` in `HopperBulletEnv` parent class.\"\"\"\n",
    "    self.episode_steps = episode_steps\n",
    "    # self.start_pos_x, self.start_pos_y, self.start_pos_z = 0, 0, 0\n",
    "    # self.body_xyz = [28, -20, 5]\n",
    "    self.basePosition = [28, -20, 5]\n",
    "    super().__init__(render=render)\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"Modifies `reset` in `WalkerBaseBulletEnv` base class.\"\"\"\n",
    "    self.step_counter = 0\n",
    "    return super().reset()\n",
    "\n",
    "  def _isDone(self):\n",
    "    \"\"\"Modifies `_isDone` in `WalkerBaseBulletEnv` base class.\"\"\"\n",
    "    return (self.step_counter == self.episode_steps\n",
    "            or super()._isDone())\n",
    "\n",
    "  def step(self, a):\n",
    "    \"\"\"Fully overrides `step` in `WalkerBaseBulletEnv` base class.\"\"\"\n",
    "\n",
    "    self.step_counter += 1\n",
    "\n",
    "    # if multiplayer, action first applied to all robots,\n",
    "    # then global step() called, then _step() for all robots\n",
    "    # with the same actions\n",
    "    if not self.scene.multiplayer:\n",
    "      self.robot.apply_action(a)\n",
    "      self.scene.global_step()\n",
    "\n",
    "    state = self.robot.calc_state()  # also calculates self.joints_at_limit\n",
    "\n",
    "    # state[0] is body height above ground, body_rpy[1] is pitch\n",
    "    self._alive = float(self.robot.alive_bonus(state[0] + self.robot.initial_z,\n",
    "                                               self.robot.body_rpy[1]))\n",
    "    done = self._isDone()\n",
    "    if not np.isfinite(state).all():\n",
    "      print(\"~INF~\", state)\n",
    "      done = True\n",
    "\n",
    "    potential_old = self.potential\n",
    "    self.potential = self.robot.calc_potential()\n",
    "    progress = float(self.potential - potential_old)\n",
    "\n",
    "    feet_collision_cost = 0.0\n",
    "    for i, f in enumerate(self.robot.feet):\n",
    "      contact_ids = set((x[2], x[4]) for x in f.contact_list())\n",
    "      if (self.ground_ids & contact_ids):\n",
    "        self.robot.feet_contact[i] = 1.0\n",
    "      else:\n",
    "        self.robot.feet_contact[i] = 0.0\n",
    "\n",
    "    # let's assume we have DC motor with controller, and reverse current braking\n",
    "    electricity_cost = self.electricity_cost * float(\n",
    "        np.abs(a * self.robot.joint_speeds).mean())\n",
    "    electricity_cost += self.stall_torque_cost * float(np.square(a).mean())\n",
    "\n",
    "    joints_at_limit_cost = float(self.joints_at_limit_cost * self.robot.joints_at_limit)\n",
    "\n",
    "    self.rewards = [\n",
    "                    self._alive, progress, electricity_cost,\n",
    "                    joints_at_limit_cost, feet_collision_cost\n",
    "                    ]\n",
    "    self.HUD(state, a, done)\n",
    "    self.reward += sum(self.rewards)\n",
    "\n",
    "    return state, sum(self.rewards), bool(done), {}\n",
    "env = HumanoidURDF()\n",
    "env = wrappers.GymWrapper(env)\n",
    "env = wrappers.SinglePrecisionWrapper(env)\n",
    "action_spec = env.action_spec()  # Specifies action shape and dimensions.\n",
    "env_spec = specs.make_environment_spec(env)\n",
    "print(env.robot.walk_target_x, env.robot.walk_target_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce5d9e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = env.reset()\n",
    "\n",
    "frame = env.environment.render(mode='rgb_array')\n",
    "plt.imshow(frame)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10bbad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 200\n",
    "\n",
    "frames = []\n",
    "timestep = env.reset()\n",
    "for _ in range(n_steps):\n",
    "  # Random control of actuators.\n",
    "  action = np.random.uniform(action_spec.minimum,\n",
    "                             action_spec.maximum,\n",
    "                             size=action_spec.shape)\n",
    "  timestep = env.step(action)\n",
    "  frames.append(env.environment.render(mode='rgb_array'))\n",
    "\n",
    "display_video(frames, framerate=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aac5685",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(action_spec.maximum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a785b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Actions:\\n', env_spec.actions)\n",
    "print('Observations:\\n', env_spec.observations)\n",
    "print('Rewards:\\n', env_spec.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6ddd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "for body_part in env.robot.parts.keys():\n",
    "  print(f\"{body_part:10} {env.robot.parts[body_part].pose().xyz()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3040ac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.robot_body.speed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17225b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_log_every = 60.  # Learner logging frequency, seconds.\n",
    "loop_log_every = 60.  # Environment loop logging frequency, seconds.\n",
    "checkpoint = True  # Checkpoint saved every 10 minutes.\n",
    "\n",
    "learner_logger = loggers.TerminalLogger(label='Learner',\n",
    "                                        time_delta=learner_log_every,\n",
    "                                        print_fn=print)\n",
    "loop_logger = loggers.TerminalLogger(label='Environment Loop',\n",
    "                                     time_delta=loop_log_every,\n",
    "                                     print_fn=print)\n",
    "\n",
    "# Note: optimizers can be passed only to the D4PG and DMPO agents.\n",
    "# The optimizer for DDPG is hard-coded in the agent class.\n",
    "policy_optimizer = snt.optimizers.Adam(1e-4)\n",
    "critic_optimizer = snt.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a225257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create networks.\n",
    "policy_network, critic_network = make_networks_d4pg(action_spec)\n",
    "\n",
    "# Create agent.\n",
    "agent = D4PG(environment_spec=env_spec,\n",
    "             policy_network=policy_network,\n",
    "             critic_network=critic_network,\n",
    "             observation_network=tf2_utils.batch_concat, # Identity Op.\n",
    "             policy_optimizer=policy_optimizer,\n",
    "             critic_optimizer=critic_optimizer,\n",
    "             logger=learner_logger,\n",
    "             checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5077587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 1000000 # 100_000  # Number of environment loop steps. Adjust as needed!\n",
    "\n",
    "loop = environment_loop.EnvironmentLoop(env, agent, logger=loop_logger)\n",
    "\n",
    "# Start training!\n",
    "loop.run(num_episodes=None,\n",
    "         num_steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36064278",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_step = 0\n",
    "for episode in reward:\n",
    "  plt.plot(np.arange(env_step, env_step+len(episode)), episode)\n",
    "  env_step += len(episode)\n",
    "plt.xlabel('Timestep', fontsize=14)\n",
    "plt.ylabel('Reward', fontsize=14)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f687d688",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, episode in enumerate(reward):\n",
    "  print(f\"Total reward in episode {i}: {sum(episode):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070fdd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the environment with the learned policy and display video.\n",
    "n_steps = 500\n",
    "\n",
    "frames = []  # Frames for video.\n",
    "reward = [[]]  # Reward at every timestep.\n",
    "timestep = env.reset()\n",
    "for _ in range(n_steps):\n",
    "  frames.append(env.environment.render(mode='rgb_array').copy())\n",
    "  action = agent.select_action(timestep.observation)\n",
    "  timestep = env.step(action)\n",
    "\n",
    "  # `timestep.reward` is None when episode terminates.\n",
    "  if timestep.reward:\n",
    "    # Old episode continues.\n",
    "    reward[-1].append(timestep.reward.item())\n",
    "  else:\n",
    "    # New episode begins.\n",
    "    reward.append([])\n",
    "\n",
    "display_video(frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c729b17",
   "metadata": {},
   "source": [
    "// Oh wow! Documentation\n",
    "\n",
    "+ http://mujoco.org/book/modeling.html\n",
    "\n",
    "+ http://www.mujoco.org/book/XMLreference.html\n",
    "\n",
    "+ https://usermanual.wiki/Document/pybullet20quickstart20guide.479068914/html\n",
    "\n",
    "+ https://github.com/bulletphysics/bullet3/tree/master/examples/pybullet/gym\n",
    "\n",
    "\n",
    "To add:\n",
    "+ move_robot in WalkerBaseBulletEnv to HumanoidInitialize\n",
    "+ To change checkpoints go to: flag_reposition function in robot_locomotors in class HumanoidFlagrun\n",
    "+ reset_positon in robot_bases\n",
    "+ move_and_look_at in gym_locomotion_envs for changing camera postion or camera_adjust WalkerBaseBulletEnv in gym_locomotion_envs\n",
    "+ create r2d2 flags\n",
    "+ take bot from just flagrun and not harder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "402f7f3e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['- HumanoidDeepMimicBackflipBulletEnv-v1', '- HumanoidDeepMimicWalkBulletEnv-v1', '- CartPoleBulletEnv-v1', '- CartPoleContinuousBulletEnv-v0', '- MinitaurBulletEnv-v0', '- MinitaurBulletDuckEnv-v0', '- RacecarBulletEnv-v0', '- RacecarZedBulletEnv-v0', '- KukaBulletEnv-v0', '- KukaCamBulletEnv-v0', '- InvertedPendulumBulletEnv-v0', '- InvertedDoublePendulumBulletEnv-v0', '- InvertedPendulumSwingupBulletEnv-v0', '- ReacherBulletEnv-v0', '- PusherBulletEnv-v0', '- ThrowerBulletEnv-v0', '- Walker2DBulletEnv-v0', '- HalfCheetahBulletEnv-v0', '- AntBulletEnv-v0', '- HopperBulletEnv-v0', '- HumanoidBulletEnv-v0', '- HumanoidFlagrunBulletEnv-v0', '- HumanoidFlagrunHarderBulletEnv-v0', '- MinitaurExtendedEnv-v0', '- MinitaurReactiveEnv-v0', '- MinitaurBallGymEnv-v0', '- MinitaurTrottingEnv-v0', '- MinitaurStandGymEnv-v0', '- MinitaurAlternatingLegsEnv-v0', '- MinitaurFourLegStandEnv-v0', '- KukaDiverseObjectGrasping-v0']\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib\n",
    "import pybullet_envs2\n",
    "import pybullet as _p\n",
    "\n",
    "from acme.utils import loggers\n",
    "from acme.tf import networks\n",
    "from acme.tf import utils as tf2_utils\n",
    "from acme.agents.tf.d4pg import D4PG\n",
    "from acme.agents.tf.ddpg import DDPG\n",
    "from acme.agents.tf.dmpo import DistributionalMPO\n",
    "from acme import wrappers, specs, environment_loop\n",
    "\n",
    "import numpy as np\n",
    "import sonnet as snt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# from google.colab import drive\n",
    "from IPython.display import HTML\n",
    "\n",
    "from pybullet_envs2.gym_locomotion_envs import HopperBulletEnv\n",
    "from pybullet_envs2.gym_locomotion_envs import Walker2DBulletEnv\n",
    "from pybullet_envs2.gym_locomotion_envs import HalfCheetahBulletEnv\n",
    "from pybullet_envs2.gym_locomotion_envs import AntBulletEnv\n",
    "from pybullet_envs2.gym_locomotion_envs import AntBulletEnv\n",
    "from pybullet_envs2.gym_locomotion_envs import HumanoidFlagrunBulletEnv, HumanoidFlagrunHarderBulletEnv\n",
    "\n",
    "from pybullet_envs2.robot_locomotors import *\n",
    "\n",
    "from pybullet_utils import bullet_client\n",
    "# perfect our own instance of the enviroment is created\n",
    "# time to manipulate the environments\n",
    "print(pybullet_envs2.getList())\n",
    "\n",
    "# @title Figure settings\n",
    "import ipywidgets as widgets       # interactive display\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle\")\n",
    "\n",
    "# np.random.seed(seed=3)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e682f6d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/vanillaskies/projects/nma2/robolympics-git2/pybullet_data2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pybullet_data2\n",
    "pybullet_data2.getDataPath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62870f55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target position: 36.90058899 -3.27299237\n",
      "1000.0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vanillaskies/anaconda3/envs/mne/lib/python3.9/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "def save_ckpt_to_drive(agent):\n",
    "  \"\"\"Saves agent checkpoint directory to Google Drive.\n",
    "\n",
    "  WARNING: Will replace the entire content of the\n",
    "  drive directory `/root/drive/MyDrive/acme_ckpt`.\n",
    "\n",
    "  Args:\n",
    "    agent: core.Actor\n",
    "  \"\"\"\n",
    "  src = agent._learner._checkpointer._checkpoint_manager.directory\n",
    "  dst = '/root/drive/MyDrive/acme_ckpt'\n",
    "  if os.path.exists(dst):\n",
    "    shutil.rmtree(dst)\n",
    "  shutil.copytree(src, dst)\n",
    "  print(f'Saved {src} to {dst}')\n",
    "\n",
    "\n",
    "def restore_ckpt_from_drive(agent):\n",
    "  \"\"\"Restores agent checkpoint directory from Google Drive.\n",
    "\n",
    "  The name of the local checkpoint directory will be different\n",
    "  than it was when the checkpoint was originally saved.\n",
    "  This is because `acme` checkpoiner creates a new directory\n",
    "  upon restart.\n",
    "\n",
    "  WARNING: Will replace the entire content of the local\n",
    "  checkpoint directory (if it exists already).\n",
    "\n",
    "  Args:\n",
    "    agent: core.Actor\n",
    "  \"\"\"\n",
    "  src = '/root/drive/MyDrive/acme_ckpt'\n",
    "  dst = agent._learner._checkpointer._checkpoint_manager.directory\n",
    "  if os.path.exists(dst):\n",
    "        shutil.rmtree(dst)\n",
    "  shutil.copytree(src, dst)\n",
    "  print(f'Restored {dst} from {src}')\n",
    "    \n",
    "def display_video(frames, framerate=30):\n",
    "  \"\"\"Generates video from `frames`.\n",
    "\n",
    "  Args:\n",
    "    frames (ndarray): Array of shape (n_frames, height, width, 3).\n",
    "    framerate (int): Frame rate in units of Hz.\n",
    "\n",
    "  Returns:\n",
    "    Display object.\n",
    "  \"\"\"\n",
    "  height, width, _ = frames[0].shape\n",
    "  dpi = 70\n",
    "  orig_backend = matplotlib.get_backend()\n",
    "  matplotlib.use('Agg')  # Switch to headless 'Agg' to inhibit figure rendering.\n",
    "  fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
    "  matplotlib.use(orig_backend)  # Switch back to the original backend.\n",
    "  ax.set_axis_off()\n",
    "  ax.set_aspect('equal')\n",
    "  ax.set_position([0, 0, 1, 1])\n",
    "  im = ax.imshow(frames[0])\n",
    "  def update(frame):\n",
    "    im.set_data(frame)\n",
    "    return [im]\n",
    "  interval = 1000/framerate\n",
    "  anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
    "                                  interval=interval, blit=True, repeat=False)\n",
    "  return HTML(anim.to_html5_video())\n",
    "\n",
    "def make_networks_d4pg(action_spec,\n",
    "                       policy_layer_sizes=(256, 256, 256),\n",
    "                       critic_layer_sizes=(512, 512, 256),\n",
    "                       vmin=-150.,\n",
    "                       vmax=150.,\n",
    "                       num_atoms=51,\n",
    "                      ):\n",
    "  \"\"\"Networks for D4PG agent.\"\"\"\n",
    "  action_size = np.prod(action_spec.shape, dtype=int)\n",
    "\n",
    "  policy_network = snt.Sequential([\n",
    "      tf2_utils.batch_concat,\n",
    "      networks.LayerNormMLP(layer_sizes=policy_layer_sizes + (action_size,)),\n",
    "      networks.TanhToSpec(spec=action_spec)\n",
    "      ])\n",
    "  critic_network = snt.Sequential([\n",
    "      networks.CriticMultiplexer(\n",
    "          action_network=networks.ClipToSpec(action_spec),\n",
    "          critic_network=networks.LayerNormMLP(\n",
    "              layer_sizes=critic_layer_sizes,\n",
    "              activate_final=True),\n",
    "      ),\n",
    "      networks.DiscreteValuedHead(vmin=vmin,\n",
    "                                  vmax=vmax,\n",
    "                                  num_atoms=num_atoms)\n",
    "      ])\n",
    "\n",
    "  return policy_network, critic_network\n",
    "\n",
    "\n",
    "def make_networks_ddpg(action_spec,\n",
    "                       policy_layer_sizes=(256, 256, 256),\n",
    "                       critic_layer_sizes=(512, 512, 256),\n",
    "                      ):\n",
    "  \"\"\"Networks for DDPG agent.\"\"\"\n",
    "  action_size = np.prod(action_spec.shape, dtype=int)\n",
    "\n",
    "  policy_network = snt.Sequential([\n",
    "      tf2_utils.batch_concat,\n",
    "      networks.LayerNormMLP(layer_sizes=policy_layer_sizes + (action_size,)),\n",
    "      networks.TanhToSpec(spec=action_spec)\n",
    "      ])\n",
    "  critic_network = networks.CriticMultiplexer(\n",
    "          action_network=networks.ClipToSpec(action_spec),\n",
    "          critic_network=networks.LayerNormMLP(\n",
    "              layer_sizes=critic_layer_sizes + (1,),\n",
    "              activate_final=False),\n",
    "              )\n",
    "\n",
    "  return policy_network, critic_network\n",
    "\n",
    "\n",
    "def make_networks_dmpo(action_spec,\n",
    "                       policy_layer_sizes=(256, 256, 256),\n",
    "                       critic_layer_sizes=(512, 512, 256),\n",
    "                       vmin=-150.,\n",
    "                       vmax=150.,\n",
    "                       num_atoms=51,\n",
    "                      ):\n",
    "  \"\"\"Networks for DMPO agent.\"\"\"\n",
    "  action_size = np.prod(action_spec.shape, dtype=int)\n",
    "\n",
    "  policy_network = snt.Sequential([\n",
    "      tf2_utils.batch_concat,\n",
    "      networks.LayerNormMLP(layer_sizes=policy_layer_sizes,\n",
    "                            activate_final=True),\n",
    "      networks.MultivariateNormalDiagHead(\n",
    "          action_size,\n",
    "          min_scale=1e-6,\n",
    "          tanh_mean=False,\n",
    "          init_scale=0.7,\n",
    "          fixed_scale=False,\n",
    "          use_tfd_independent=True)\n",
    "  ])\n",
    "\n",
    "  # The multiplexer concatenates the (maybe transformed) observations/actions.\n",
    "  critic_network = networks.CriticMultiplexer(\n",
    "      action_network=networks.ClipToSpec(action_spec),\n",
    "      critic_network=networks.LayerNormMLP(layer_sizes=critic_layer_sizes,\n",
    "                                           activate_final=True),\n",
    "                                           )\n",
    "  critic_network = snt.Sequential([\n",
    "                                   critic_network,\n",
    "                                   networks.DiscreteValuedHead(vmin=vmin,\n",
    "                                                               vmax=vmax,\n",
    "                                                               num_atoms=num_atoms)\n",
    "                                   ])\n",
    "\n",
    "  return policy_network, critic_network\n",
    "\n",
    "\n",
    "checkpoints_stadium = np.array([[36.90058899,  -3.27299237],\n",
    "       [ 29.98545074, -15.73727989],\n",
    "       [ 25.98292923,  19.71592522],\n",
    "       [ 36.00297928,   7.93183422],\n",
    "       [-36.00297928,  -6.91670513],\n",
    "       [-28.69959831,  17.92435074],\n",
    "       [-34.2795639 ,  11.62551594],\n",
    "       [ 37.12164688,   2.41725087],\n",
    "       [-32.95801163, -12.49569416],\n",
    "       [-36.90058899,   4.28812122],\n",
    "       [ 35.49053192,  -7.93686962],\n",
    "       [ 26.26033592, -18.4656353 ],\n",
    "       [-17.51765633,  22.4084053 ],\n",
    "       [ 33.36088181, -11.92537308],\n",
    "       [-23.28717995,  20.93785286],\n",
    "       [-37.12164688,  -1.40212178],\n",
    "       [ 24.69773293, -19.26901054],\n",
    "       [ 30.10389709,  16.65581703],\n",
    "       [-28.98753738, -16.69599724],\n",
    "       [ 32.958004  ,  13.51084328],\n",
    "       [-24.59379959, -19.27071381],\n",
    "       [ 18.29857635,  22.3815155 ],\n",
    "       [ 17.46794701, -21.23395348],\n",
    "       [-20.45232391, -20.74229622],\n",
    "       [-17.62427711, -21.17273331],\n",
    "       [ 23.09788132, -19.88587952],\n",
    "       [-16.22319412, -21.3013134 ],\n",
    "       [-26.827631  , -21.17273331]])\n",
    "\n",
    "counter = 0\n",
    "\n",
    "class HumanoidInitialize(HumanoidFlagrunBulletEnv):\n",
    "  def __init__(self, render=False, episode_steps=1000):\n",
    "    \"\"\"Modifies `__init__` in `HopperBulletEnv` parent class.\"\"\"\n",
    "    self._p = bullet_client.BulletClient()\n",
    "    self.episode_steps = episode_steps\n",
    "    global counter\n",
    "    self.walk_target_x = checkpoints_stadium[counter, 0]\n",
    "    self.walk_target_y = checkpoints_stadium[counter, 1]\n",
    "    counter += 1\n",
    "    print(\"target position:\", self.walk_target_x, self.walk_target_y)\n",
    "    super().__init__(render=render)\n",
    "    \n",
    "  def reset(self):\n",
    "    \"\"\"Modifies `reset` in `WalkerBaseBulletEnv` base class.\"\"\"\n",
    "    self.step_counter = 0\n",
    "    self.set_checkpoints()\n",
    "    return super().reset()\n",
    "\n",
    "  def _isDone(self):\n",
    "    \"\"\"Modifies `_isDone` in `WalkerBaseBulletEnv` base class.\"\"\"\n",
    "    return (self.step_counter == self.episode_steps\n",
    "            or super()._isDone())\n",
    "  \n",
    "  def set_checkpoints(self):\n",
    "    for checkpoint in range(0, checkpoints_stadium.shape[0]):\n",
    "      print(\"setting checkpoint:\", checkpoint)\n",
    "      flag_r2d2 = self._p.loadURDF(os.path.join(pybullet_data.getDataPath(), \"r2d2.urdf\"), [checkpoints_stadium[checkpoint, 0], checkpoints_stadium[checkpoint, 1], 0])\n",
    "  \n",
    "  def step(self, a):\n",
    "    \"\"\"Fully overrides `step` in `WalkerBaseBulletEnv` base class.\"\"\"\n",
    "\n",
    "    self.step_counter += 1\n",
    "\n",
    "    # if multiplayer, action first applied to all robots,\n",
    "    # then global step() called, then _step() for all robots\n",
    "    # with the same actions\n",
    "    if not self.scene.multiplayer:\n",
    "      self.robot.apply_action(a)\n",
    "      self.scene.global_step()\n",
    "\n",
    "    state = self.robot.calc_state()  # also calculates self.joints_at_limit\n",
    "\n",
    "    # state[0] is body height above ground, body_rpy[1] is pitch\n",
    "    self._alive = float(self.robot.alive_bonus(state[0] + self.robot.initial_z,\n",
    "                                               self.robot.body_rpy[1]))\n",
    "    done = self._isDone()\n",
    "    if not np.isfinite(state).all():\n",
    "      print(\"~INF~\", state)\n",
    "      done = True\n",
    "\n",
    "    potential_old = self.potential\n",
    "    self.potential = self.robot.calc_potential()\n",
    "    progress = float(self.potential - potential_old)\n",
    "\n",
    "    feet_collision_cost = 0.0\n",
    "    for i, f in enumerate(self.robot.feet):\n",
    "      contact_ids = set((x[2], x[4]) for x in f.contact_list())\n",
    "      if (self.ground_ids & contact_ids):\n",
    "        self.robot.feet_contact[i] = 1.0\n",
    "      else:\n",
    "        self.robot.feet_contact[i] = 0.0\n",
    "\n",
    "    # let's assume we have DC motor with controller, and reverse current braking\n",
    "    electricity_cost = self.electricity_cost * float(\n",
    "        np.abs(a * self.robot.joint_speeds).mean())\n",
    "    electricity_cost += self.stall_torque_cost * float(np.square(a).mean())\n",
    "\n",
    "    joints_at_limit_cost = float(self.joints_at_limit_cost * self.robot.joints_at_limit)\n",
    "\n",
    "    self.rewards = [\n",
    "                    self._alive, progress, electricity_cost,\n",
    "                    joints_at_limit_cost, feet_collision_cost\n",
    "                    ]\n",
    "    self.HUD(state, a, done)\n",
    "    self.reward += sum(self.rewards)\n",
    "\n",
    "    return state, sum(self.rewards), bool(done), {}\n",
    "  \n",
    "  def flag_reposition(self, x_index, y_index):\n",
    "    self.walk_target_x = checkpoints_stadium[x_index, y_index]\n",
    "    self.walk_target_y = checkpoints_stadium[x_index, y_index+1]\n",
    "\n",
    "\n",
    "    if (self.flag):\n",
    "      for b in self.flag.bodies:\n",
    "        print(\"remove body uid\",b)\n",
    "        # p.removeBody(b)\n",
    "      self._p.resetBasePositionAndOrientation(self.flag.bodies[0],\n",
    "                                              [self.walk_target_x, self.walk_target_y, 0.7],\n",
    "                                              [0, 0, 0, 1])\n",
    "      self.flag = get_sphere(self._p, self.walk_target_x, self.walk_target_y, 0.7)\n",
    "    else:\n",
    "      self.flag = get_sphere(self._p, self.walk_target_x, self.walk_target_y, 0.7)\n",
    "    self.flag_timeout = 600 / self.scene.frame_skip  #match Roboschool\n",
    "\n",
    "  def calc_state(self):\n",
    "    self.flag_timeout -= 1\n",
    "    state = Humanoid.calc_state(self)\n",
    "    if self.walk_target_dist < 1 or self.flag_timeout <= 0:\n",
    "      self.flag_reposition()\n",
    "      state = Humanoid.calc_state(self)  # caclulate state again, against new flag pos\n",
    "      self.potential = self.calc_potential()  # avoid reward jump\n",
    "    return state\n",
    "\n",
    "env = HumanoidInitialize()\n",
    "env = wrappers.GymWrapper(env)\n",
    "env = wrappers.SinglePrecisionWrapper(env)\n",
    "action_spec = env.action_spec()  # Specifies action shape and dimensions.\n",
    "env_spec = specs.make_environment_spec(env)\n",
    "print(env.robot.walk_target_x, env.robot.walk_target_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4015ffa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pybullet path now:  /home/vanillaskies/projects/nma2/robolympics-git2/pybullet_data2\n"
     ]
    }
   ],
   "source": [
    "from pybullet_envs2.robot_bases import check_pybullet_path\n",
    "check_pybullet_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ea82aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88920096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoints_stadium.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80d6a2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting checkpoint: 0\n",
      "setting checkpoint: 1\n",
      "setting checkpoint: 2\n",
      "setting checkpoint: 3\n",
      "setting checkpoint: 4\n",
      "setting checkpoint: 5\n",
      "setting checkpoint: 6\n",
      "setting checkpoint: 7\n",
      "setting checkpoint: 8\n",
      "setting checkpoint: 9\n",
      "setting checkpoint: 10\n",
      "setting checkpoint: 11\n",
      "setting checkpoint: 12\n",
      "setting checkpoint: 13\n",
      "setting checkpoint: 14\n",
      "setting checkpoint: 15\n",
      "setting checkpoint: 16\n",
      "setting checkpoint: 17\n",
      "setting checkpoint: 18\n",
      "setting checkpoint: 19\n",
      "setting checkpoint: 20\n",
      "setting checkpoint: 21\n",
      "setting checkpoint: 22\n",
      "setting checkpoint: 23\n",
      "setting checkpoint: 24\n",
      "setting checkpoint: 25\n",
      "setting checkpoint: 26\n",
      "setting checkpoint: 27\n"
     ]
    }
   ],
   "source": [
    "env.set_checkpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ce5d9e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HumanoidInitialize' object has no attribute 'robot_body'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a9cf3e28837d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# _ = env.reset()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobot_body\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobot_body\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m26\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# env.flag.reset_position([28, -20, 5])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# env.camera.move_and_look_at(i=10, j=20, x=100, y=120, z=100, k=1.5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mne/lib/python3.9/site-packages/acme/wrappers/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Delegates attribute calls to the wrapped environment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_environment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mne/lib/python3.9/site-packages/acme/wrappers/gym_wrapper.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     78\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# Expose any other attributes of the underlying environment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_environment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HumanoidInitialize' object has no attribute 'robot_body'"
     ]
    }
   ],
   "source": [
    "# _ = env.reset()\n",
    "print(env.robot_body.current_position())\n",
    "env.robot_body.reset_position(position=[26, -15, 3])\n",
    "# env.flag.reset_position([28, -20, 5])\n",
    "# env.camera.move_and_look_at(i=10, j=20, x=100, y=120, z=100, k=1.5)\n",
    "# env.flag.\n",
    "\n",
    "# can see the checkpoints now, but can't reset env\n",
    "# for checkpoint in range(0, checkpoints_stadium.shape[0]):\n",
    "#     print(\"setting checkpoint:\", checkpoint)\n",
    "#     flag_r2d2 = env._p.loadURDF(os.path.join(pybullet_data.getDataPath(), \"r2d2.urdf\"), [checkpoints_stadium[checkpoint, 0], checkpoints_stadium[checkpoint, 1], 0])\n",
    "frame = env.environment.render(mode='rgb_array')\n",
    "plt.imshow(frame)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.flag.current_position()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cb9f5f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HumanoidInitialize' object has no attribute 'robot_body'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-97713ebed728>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobot_body\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/mne/lib/python3.9/site-packages/acme/wrappers/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Delegates attribute calls to the wrapped environment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_environment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mne/lib/python3.9/site-packages/acme/wrappers/gym_wrapper.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     78\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# Expose any other attributes of the underlying environment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_environment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HumanoidInitialize' object has no attribute 'robot_body'"
     ]
    }
   ],
   "source": [
    "env.robot_body.current_position()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acff695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "HumanoidInitialize.flag_reposition(HumanoidInitialize, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "845edf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_environment': <acme.wrappers.gym_wrapper.GymWrapper object at 0x7fc865956c10>}\n"
     ]
    }
   ],
   "source": [
    "print(env.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48d62acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__module__': '__main__', '__init__': <function HumanoidInitialize.__init__ at 0x7fc859113f70>, 'reset': <function HumanoidInitialize.reset at 0x7fc859137550>, '_isDone': <function HumanoidInitialize._isDone at 0x7fc8591375e0>, 'set_checkpoints': <function HumanoidInitialize.set_checkpoints at 0x7fc859137670>, 'step': <function HumanoidInitialize.step at 0x7fc859137700>, 'flag_reposition': <function HumanoidInitialize.flag_reposition at 0x7fc859137790>, 'calc_state': <function HumanoidInitialize.calc_state at 0x7fc859137820>, '__doc__': None}\n"
     ]
    }
   ],
   "source": [
    "print(HumanoidInitialize.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d869b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# camera not resetting\n",
    "\n",
    "camInfo_now = env.camera.get_camera_location()\n",
    "print(camInfo_now)\n",
    "env.camera.move_and_look_at(i=10, j=20, x=100, y=120, z=100, k=1.5)\n",
    "camInfo_now = env.camera.get_camera_location()\n",
    "print(camInfo_now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10bbad3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_steps = 200\n",
    "\n",
    "frames = []\n",
    "timestep = env.reset()\n",
    "for _ in range(n_steps):\n",
    "  # Random control of actuators.\n",
    "  #env.robot_body.reset_position(position=[28, -20, 5])\n",
    "  env.robot_body.reset_position(position=[26, -20, 3])\n",
    "  env.robot.walk_target_x = checkpoints_stadium[0, 0]\n",
    "  env.robot.walk_target_x = checkpoints_stadium[0, 1]\n",
    "  # print(env.robot_body.current_position())\n",
    "  action = np.random.uniform(action_spec.minimum,\n",
    "                             action_spec.maximum,\n",
    "                             size=action_spec.shape)\n",
    "  timestep = env.step(action)\n",
    "  frames.append(env.environment.render(mode='rgb_array'))\n",
    "\n",
    "display_video(frames, framerate=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a24008",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(action_spec.minimum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a785b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Actions:\\n', env_spec.actions)\n",
    "print('\\nObservations:\\n', env_spec.observations)\n",
    "print('\\nRewards:\\n', env_spec.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6ddd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "for body_part in env.robot.parts.keys():\n",
    "  print(f\"{body_part:10} {env.robot.parts[body_part].pose().xyz()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3040ac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.robot_body.speed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17225b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_log_every = 60.  # Learner logging frequency, seconds.\n",
    "loop_log_every = 60.  # Environment loop logging frequency, seconds.\n",
    "checkpoint = True  # Checkpoint saved every 10 minutes.\n",
    "\n",
    "learner_logger = loggers.TerminalLogger(label='Learner',\n",
    "                                        time_delta=learner_log_every,\n",
    "                                        print_fn=print)\n",
    "loop_logger = loggers.TerminalLogger(label='Environment Loop',\n",
    "                                     time_delta=loop_log_every,\n",
    "                                     print_fn=print)\n",
    "\n",
    "# Note: optimizers can be passed only to the D4PG and DMPO agents.\n",
    "# The optimizer for DDPG is hard-coded in the agent class.\n",
    "policy_optimizer = snt.optimizers.Adam(1e-4)\n",
    "critic_optimizer = snt.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a225257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create networks.\n",
    "policy_network, critic_network = make_networks_d4pg(action_spec)\n",
    "\n",
    "# Create agent.\n",
    "agent = D4PG(environment_spec=env_spec,\n",
    "             policy_network=policy_network,\n",
    "             critic_network=critic_network,\n",
    "             observation_network=tf2_utils.batch_concat, # Identity Op.\n",
    "             policy_optimizer=policy_optimizer,\n",
    "             critic_optimizer=critic_optimizer,\n",
    "             logger=learner_logger,\n",
    "             checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5077587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 100# 100_000  # Number of environment loop steps. Adjust as needed!\n",
    "\n",
    "loop = environment_loop.EnvironmentLoop(env, agent, logger=loop_logger)\n",
    "\n",
    "# Start training!\n",
    "loop.run(num_episodes=None,\n",
    "         num_steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea23687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the environment with the learned policy and display video.\n",
    "n_steps = 500\n",
    "\n",
    "frames = []  # Frames for video.\n",
    "reward = [[]]  # Reward at every timestep.\n",
    "timestep = env.reset()\n",
    "for _ in range(n_steps):\n",
    "  frames.append(env.environment.render(mode='rgb_array').copy())\n",
    "  action = agent.select_action(timestep.observation)\n",
    "  timestep = env.step(action)\n",
    "\n",
    "  # `timestep.reward` is None when episode terminates.\n",
    "  if timestep.reward:\n",
    "    # Old episode continues.\n",
    "    reward[-1].append(timestep.reward.item())\n",
    "  else:\n",
    "    # New episode begins.\n",
    "    reward.append([])\n",
    "\n",
    "display_video(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36064278",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_step = 0\n",
    "for episode in reward:\n",
    "  plt.plot(np.arange(env_step, env_step+len(episode)), episode)\n",
    "  env_step += len(episode)\n",
    "plt.xlabel('Timestep', fontsize=14)\n",
    "plt.ylabel('Reward', fontsize=14)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f687d688",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, episode in enumerate(reward):\n",
    "  print(f\"Total reward in episode {i}: {sum(episode):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81385bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
